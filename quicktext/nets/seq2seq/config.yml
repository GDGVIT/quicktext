# Configuration for Seq2seq with attention Model

embed_size: 300
hidden_size: 10
hidden_layers: 1
bidirectional: True
vocab_size: 26000
dropout_keep: 0.8
lr: 0.5